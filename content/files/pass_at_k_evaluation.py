# -*- coding: utf-8 -*-
"""pass_at_k_evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MGbGgy5sELjygwI4byzh_cabc9fTZlXZ
"""

!pip install javalang

#!/usr/bin/env python3
"""
PASS@1 / PASS@3 / PASS@5 EVALUATION (FINAL, LANGUAGE-WISE, NON-INFLATING)

- Each metric increments at most once per instance
- AST ⊃ Compile ⊃ Functional hierarchy preserved
- Pass@1 = greedy only
- Pass@3 = greedy + candidate_1..3
- Pass@5 = greedy + candidate_1..5
- Live stats printed every N instances
- Final per-language + global summary
"""

# =====================================================
# IMPORTS
# =====================================================

import json
import subprocess
import tempfile
import os
import shutil
import javalang
from collections import defaultdict

# =====================================================
# CONFIG
# =====================================================

INPUT_JSONL = "/content/FFT_S3_predictions.jsonl"
PROBLEM_TESTS_FILE = "/content/problem_tests.json"

EXEC_TIMEOUT = 5
PROGRESS_INTERVAL = 10


def count_dataset_instances(path):
    with open(path, "r", encoding="utf-8") as f:
        return sum(1 for _ in f)


# =====================================================
# LOAD TEST CASES
# =====================================================

def load_tests(path):
    tests = {}
    with open(path, "r", encoding="utf-8") as f:
        for item in json.load(f):
            tests[item["problem_id"]] = {
                "input": item["input"],
                "output": item["output"].strip()
            }
    return tests

# =====================================================
# JAVA UTILS
# =====================================================

def java_parsable(code):
    try:
        javalang.parse.parse(code)
        return True
    except Exception:
        return False


def java_compiles(code):
    tmpdir = tempfile.mkdtemp()
    java_file = os.path.join(tmpdir, "Main.java")

    with open(java_file, "w", encoding="utf-8") as f:
        f.write(code)

    proc = subprocess.run(
        ["javac", java_file],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    )

    if proc.returncode != 0:
        shutil.rmtree(tmpdir)
        return False, None

    return True, tmpdir


def run_java(tmpdir, test_input):
    try:
        proc = subprocess.run(
            ["java", "-cp", tmpdir, "Main"],
            input=test_input,
            text=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            timeout=EXEC_TIMEOUT
        )
        return proc.stdout.strip()
    except Exception:
        return None


# =====================================================
# EVALUATE ONE CANDIDATE
# =====================================================

def evaluate_candidate(code, test):
    """
    Returns:
        (parsable, compilable, functional)
    """
    if not code or not code.strip():
        return False, False, False

    if not java_parsable(code):
        return False, False, False

    compilable, tmpdir = java_compiles(code)
    if not compilable:
        return True, False, False

    output = run_java(tmpdir, test["input"])
    shutil.rmtree(tmpdir)

    if output == test["output"]:
        return True, True, True

    return True, True, False


# =====================================================
# MAIN EVALUATION
# =====================================================

def evaluate():

    TESTS = load_tests(PROBLEM_TESTS_FILE)
    TOTAL_INSTANCES = count_dataset_instances(INPUT_JSONL)
    print(f"Total test samples: {TOTAL_INSTANCES}")

    # -------- GLOBAL COUNTERS --------
    global_stats = {
        "total": 0,
        "parsable": 0,
        "compilable": 0,
        "pass@1": 0,
        "pass@3": 0,
        "pass@5": 0
    }

    # -------- LANGUAGE-WISE COUNTERS --------
    lang_stats = defaultdict(lambda: {
        "total": 0,
        "parsable": 0,
        "compilable": 0,
        "pass@1": 0,
        "pass@3": 0,
        "pass@5": 0
    })

    with open(INPUT_JSONL, "r", encoding="utf-8") as fin:
        for idx, line in enumerate(fin, start=1):

            item = json.loads(line)
            pid = item["problem_id"]
            lang = item.get("input_language", "UNKNOWN")
            test = TESTS.get(pid)

            global_stats["total"] += 1
            lang_stats[lang]["total"] += 1

            if test is None:
                continue

            seen_parsable = False
            seen_compilable = False
            solved_at = None  # smallest k where solved

            candidates = [
                item.get("greedy"),
                item.get("candidate_1"),
                item.get("candidate_2"),
                item.get("candidate_3"),
                item.get("candidate_4"),
                item.get("candidate_5"),
            ]

            for i, code in enumerate(candidates):

                if code is None:
                    continue

                parsable, compilable, functional = evaluate_candidate(code, test)

                if parsable and not seen_parsable:
                    global_stats["parsable"] += 1
                    lang_stats[lang]["parsable"] += 1
                    seen_parsable = True

                if compilable and not seen_compilable:
                    global_stats["compilable"] += 1
                    lang_stats[lang]["compilable"] += 1
                    seen_compilable = True

                if functional:
                    solved_at = i + 1  # 1-based
                    break   # EARLY STOP (correct Pass@K behavior)

            # -------- PASS@K UPDATES (ONCE PER INSTANCE) --------
            if solved_at == 1:
                global_stats["pass@1"] += 1
                global_stats["pass@3"] += 1
                global_stats["pass@5"] += 1

                lang_stats[lang]["pass@1"] += 1
                lang_stats[lang]["pass@3"] += 1
                lang_stats[lang]["pass@5"] += 1

            elif solved_at is not None:
                if solved_at <= 3:
                    global_stats["pass@3"] += 1
                    lang_stats[lang]["pass@3"] += 1
                if solved_at <= 5:
                    global_stats["pass@5"] += 1
                    lang_stats[lang]["pass@5"] += 1

            # -------- LIVE STATS --------
            if idx % PROGRESS_INTERVAL == 0:
                t = global_stats["total"]
                print("\n" + "-" * 70)
                print(f"Processed {idx}")
                print(f"AST-parsable : {global_stats['parsable']} ({global_stats['parsable']/t*100:.2f}%)")
                print(f"Compilable   : {global_stats['compilable']} ({global_stats['compilable']/t*100:.2f}%)")
                print(f"Pass@1       : {global_stats['pass@1']} ({global_stats['pass@1']/t*100:.2f}%)")
                print(f"Pass@3       : {global_stats['pass@3']} ({global_stats['pass@3']/t*100:.2f}%)")
                print(f"Pass@5       : {global_stats['pass@5']} ({global_stats['pass@5']/t*100:.2f}%)")
                print("-" * 70)

    # =====================================================
    # FINAL PER-LANGUAGE REPORT
    # =====================================================

    print("\n" + "=" * 100)
    print("MODEL PERFORMANCE ON FULL TEST SET — PER LANGUAGE (PASS@K)")
    print("=" * 100)
    print(
        f"{'Language':<10} {'Total':>7} "
        f"{'Parsable':>12} {'Compilable':>12} "
        f"{'Pass@1':>10} {'Pass@3':>10} {'Pass@5':>10}"
    )

    for lang in sorted(lang_stats.keys()):
        s = lang_stats[lang]
        t = s["total"]

        print(
            f"{lang:<10} {t:>7} "
            f"{s['parsable']:>4} ({s['parsable']/t*100:>6.2f}%) "
            f"{s['compilable']:>4} ({s['compilable']/t*100:>6.2f}%) "
            f"{s['pass@1']:>4} ({s['pass@1']/t*100:>6.2f}%) "
            f"{s['pass@3']:>4} ({s['pass@3']/t*100:>6.2f}%) "
            f"{s['pass@5']:>4} ({s['pass@5']/t*100:>6.2f}%)"
        )

    # =====================================================
    # GLOBAL SUMMARY
    # =====================================================

    total = global_stats["total"]

    print("\n" + "=" * 100)
    print("OVERALL AVERAGE PERFORMANCE (ALL LANGUAGES COMBINED)")
    print("=" * 100)
    print(f"Total samples : {total}")
    print(f"Parsability   : {global_stats['parsable']} ({global_stats['parsable']/total*100:.2f}%)")
    print(f"Compilability : {global_stats['compilable']} ({global_stats['compilable']/total*100:.2f}%)")
    print(f"Pass@1        : {global_stats['pass@1']} ({global_stats['pass@1']/total*100:.2f}%)")
    print(f"Pass@3        : {global_stats['pass@3']} ({global_stats['pass@3']/total*100:.2f}%)")
    print(f"Pass@5        : {global_stats['pass@5']} ({global_stats['pass@5']/total*100:.2f}%)")


# =====================================================
# RUN
# =====================================================

if __name__ == "__main__":
    evaluate()