# -*- coding: utf-8 -*-
"""DeepseekCoderQS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EQDNpl6P_Aqjcv7zyHqP8Yxx5W9Uf5ha

# DeepseekCoder - Quality-Stratification of DeepseekCoder Translations
"""

!apt-get update
!apt-get install -y g++ clang

import json
import random

FILE_PATH = "/content/codenet_single_solution_deepseekcoder.jsonl"
NUM_SAMPLES = 5

with open(FILE_PATH, "r", encoding="utf-8") as f:
    lines = f.readlines()

samples = random.sample(lines, min(NUM_SAMPLES, len(lines)))

for i, line in enumerate(samples, 1):
    data = json.loads(line.strip())
    print(f"\n--- Random Instance {i} ---")
    print(json.dumps(data, indent=2, ensure_ascii=False))

import json
import re

# =========================
# CONFIG
# =========================
INPUT_JSONL = "/content/datasets/codenet_single_solution_deepseekcoder.jsonl"
OUTPUT_JSONL = "/content/datasets/codenet_single_solution_deepseekcoder_clean.jsonl"

# =========================
# FORBIDDEN PATTERNS
# =========================
FORBIDDEN_SUBSTRINGS = [
    "yield return",     # Python / C#
    ".use {",           # Kotlin
    "memset(",          # C / C++
    "sizeof(",          # C / C++
    "namespace ",       # C#
    "using System",     # C#
    "#include",         # C / C++
]

FORBIDDEN_REGEX = re.compile(
    r"\b(yield\s+return|memset|sizeof|namespace)\b"
)

# =========================
# JAVA EXTRACTION
# =========================
def extract_java(text: str):
    if not text:
        return None

    text = text.strip()

    # -------- Case 1: ```java fenced block --------
    if "```java" in text:
        text = text.split("```java", 1)[1]
        if "```" in text:
            text = text.split("```", 1)[0]
        text = text.strip()

    lines = text.splitlines()

    # -------- Find Java start --------
    start = None
    for i, line in enumerate(lines):
        s = line.strip()
        if s.startswith("import ") or s.startswith("package ") or "class " in s:
            start = i
            break

    if start is None:
        return None

    lines = lines[start:]

    # -------- Extract until LAST balanced brace --------
    brace = 0
    last_balanced = None

    for i, line in enumerate(lines):
        brace += line.count("{")
        brace -= line.count("}")
        if brace == 0 and "class " in "\n".join(lines[: i + 1]):
            last_balanced = i

    if last_balanced is None:
        return None

    code = "\n".join(lines[: last_balanced + 1]).strip()
    return code


# =========================
# JAVA VALIDATION
# =========================
def is_valid_java(code: str) -> bool:
    if not code:
        return False

    # Must contain at least one class
    if "class " not in code:
        return False

    # Forbidden patterns
    for bad in FORBIDDEN_SUBSTRINGS:
        if bad in code:
            return False

    if FORBIDDEN_REGEX.search(code):
        return False

    # Brace balance
    if code.count("{") != code.count("}"):
        return False

    # Must end with closing brace
    if not code.rstrip().endswith("}"):
        return False

    return True


# =========================
# MAIN CLEANING LOOP
# =========================
def clean_deepseekcoder():
    kept = 0
    dropped = 0

    with open(INPUT_JSONL, "r", encoding="utf-8") as fin, \
         open(OUTPUT_JSONL, "w", encoding="utf-8") as fout:

        for line in fin:
            item = json.loads(line)

            raw = item.get("deepseekcoder_translation_raw")
            cleaned = None

            java_code = extract_java(raw)
            if java_code and is_valid_java(java_code):
                cleaned = java_code
                kept += 1
            else:
                dropped += 1

            item["deepseekcoder_translation_clean"] = cleaned
            fout.write(json.dumps(item, ensure_ascii=False) + "\n")

    print("====================================")
    print("DeepSeek-Coder Cleaning Complete (v2)")
    print("====================================")
    print(f"Kept   : {kept}")
    print(f"Dropped: {dropped}")
    print(f"Output : {OUTPUT_JSONL}")


# =========================
# RUN
# =========================
if __name__ == "__main__":
    clean_deepseekcoder()

import json
import subprocess
import tempfile
import os
import shutil

# =========================
# CONFIG
# =========================

DEEPSEEK_FILE = "/content/datasets/codenet_single_solution_deepseekcoder_clean.jsonl"
PROBLEM_TESTS_FILE = "/content/datasets/problem_tests.json"
OUTPUT_JSONL = "/content/datasets/codenet_single_solution_deepseekcoder_scored.jsonl"

EXEC_TIMEOUT = 5  # seconds
PROGRESS_INTERVAL = 10

# =========================
# COUNT TOTAL INSTANCES
# =========================

def count_jsonl_lines(path):
    with open(path, "r", encoding="utf-8") as f:
        return sum(1 for _ in f)

TOTAL_INSTANCES = count_jsonl_lines(DEEPSEEK_FILE)
print(f"Total instances in file : {TOTAL_INSTANCES}")

# =========================
# LOAD TEST CASES
# =========================

def load_tests(path):
    tests = {}
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
        for item in data:
            pid = item["problem_id"]
            if pid not in tests:
                tests[pid] = {
                    "input": item["input"],
                    "output": item["output"].strip()
                }
    return tests

PROBLEM_TESTS = load_tests(PROBLEM_TESTS_FILE)

# =========================
# JAVA COMPILE / RUN
# =========================

def java_compiles(code):
    tmpdir = tempfile.mkdtemp()
    java_file = os.path.join(tmpdir, "Main.java")

    with open(java_file, "w", encoding="utf-8") as f:
        f.write(code)

    compile_proc = subprocess.run(
        ["javac", java_file],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    )

    if compile_proc.returncode != 0:
        shutil.rmtree(tmpdir)
        return False, None

    return True, tmpdir


def run_java(tmpdir, test_input):
    try:
        result = subprocess.run(
            ["java", "-cp", tmpdir, "Main"],
            input=test_input,
            text=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            timeout=EXEC_TIMEOUT
        )
        return result.stdout.strip()
    except Exception:
        return None

# =========================
# EVALUATION
# =========================

def evaluate_deepseekcoder():
    stats = {
        "total": 0,
        "score_0": 0,
        "parsable": 0,
        "compilable": 0,
        "functional": 0
    }

    with open(DEEPSEEK_FILE, "r", encoding="utf-8") as fin, \
         open(OUTPUT_JSONL, "w", encoding="utf-8") as fout:

        for idx, line in enumerate(fin, start=1):
            item = json.loads(line)
            stats["total"] += 1

            problem_id = item.get("problem_id")
            java_code = item.get("deepseekcoder_translation_clean")

            score = 0

            # ---------- Already cleaned ----------
            if not java_code:
                stats["score_0"] += 1
                item["score"] = 0
                fout.write(json.dumps(item) + "\n")
                continue

            stats["parsable"] += 1
            score = 1

            # ---------- Test lookup ----------
            test = PROBLEM_TESTS.get(problem_id)
            if test is None:
                item["score"] = score
                fout.write(json.dumps(item) + "\n")
                continue

            # ---------- Compile ----------
            compilable, tmpdir = java_compiles(java_code)

            if not compilable:
                item["score"] = score
                fout.write(json.dumps(item) + "\n")
                continue

            stats["compilable"] += 1
            score = 2

            # ---------- Functional ----------
            output = run_java(tmpdir, test["input"])
            shutil.rmtree(tmpdir)

            if output == test["output"]:
                stats["functional"] += 1
                score = 3

            item["score"] = score
            fout.write(json.dumps(item) + "\n")

            # ---------- Progress ----------
            if idx % PROGRESS_INTERVAL == 0 or idx == TOTAL_INSTANCES:
                percent = (idx / TOTAL_INSTANCES) * 100
                print("\n----------------------------------------")
                print(f"Progress: {idx}/{TOTAL_INSTANCES} ({percent:.2f}%)")
                print(f"Score 0       : {stats['score_0']}")
                print(f"Parsable      : {stats['parsable']}")
                print(f"Compilable    : {stats['compilable']}")
                print(f"Functional    : {stats['functional']}")
                print("----------------------------------------")

    # =========================
    # FINAL REPORT
    # =========================

    t = stats["total"]

    print("\n==============================")
    print("FINAL DeepSeek-Coder Java Results")
    print("==============================")
    print(f"Total samples       : {t}")
    print(f"Score 0             : {stats['score_0']}")
    print(f"Parsable            : {stats['parsable']}")
    print(f"Compilable          : {stats['compilable']}")
    print(f"Functional          : {stats['functional']}")

    if t > 0:
        print("\nPercentages:")
        print(f"Score 0      : {stats['score_0']/t:.2%}")
        print(f"Parsable     : {stats['parsable']/t:.2%}")
        print(f"Compilable   : {stats['compilable']/t:.2%}")
        print(f"Functional   : {stats['functional']/t:.2%}")

    print(f"\nScored file written to:\n{OUTPUT_JSONL}")

    return stats

# =========================
# RUN
# =========================

if __name__ == "__main__":
    evaluate_deepseekcoder()

import json
from collections import defaultdict

# =========================
# CONFIG
# =========================

SCORED_FILE = "/content/datasets/codenet_single_solution_deepseekcoder_scored.jsonl"

# =========================
# DATA STRUCTURE
# =========================

stats = defaultdict(lambda: {
    "total": 0,
    "score_0": 0,
    "parsable": 0,
    "compilable": 0,
    "functional": 0
})

# =========================
# READ & AGGREGATE
# =========================

with open(SCORED_FILE, "r", encoding="utf-8") as f:
    for line in f:
        item = json.loads(line)

        lang = item.get("input_language", "UNKNOWN")
        score = item.get("score", 0)

        s = stats[lang]
        s["total"] += 1

        if score == 0:
            s["score_0"] += 1
        if score >= 1:
            s["parsable"] += 1
        if score >= 2:
            s["compilable"] += 1
        if score == 3:
            s["functional"] += 1

# =========================
# PRINT SUMMARY
# =========================

print("\n==============================================")
print(" DeepSeek-Coder – Per Input Language Summary")
print("==============================================")

for lang in sorted(stats.keys()):
    s = stats[lang]
    t = s["total"]

    print(f"\nLanguage: {lang}")
    print("-" * (10 + len(lang)))
    print(f"Total samples   : {t}")
    print(f"Score 0         : {s['score_0']} ({s['score_0']/t:.2%})")
    print(f"Parsable (≥1)   : {s['parsable']} ({s['parsable']/t:.2%})")
    print(f"Compilable (≥2) : {s['compilable']} ({s['compilable']/t:.2%})")
    print(f"Functional (=3) : {s['functional']} ({s['functional']/t:.2%})")

# =========================
# OPTIONAL: SORT BY FUNCTIONAL RATE
# =========================

print("\n==============================================")
print(" Languages Sorted by Functional Accuracy")
print("==============================================")

ranked = sorted(
    stats.items(),
    key=lambda x: (x[1]["functional"] / x[1]["total"]) if x[1]["total"] > 0 else 0,
    reverse=True
)

for lang, s in ranked:
    rate = s["functional"] / s["total"] if s["total"] > 0 else 0
    print(f"{lang:12s} : {rate:.2%}  ({s['functional']}/{s['total']})")